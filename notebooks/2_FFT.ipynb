{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import librosa\n",
    "\n",
    "if torchaudio.get_audio_backend() == 'soundfile':\n",
    "    print(\"Warning: sox_io not available, using soundfile instead, this may cause some issue when loading .mp3\")\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from librosa.display import waveshow, specshow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT, STFT and MelSpectrogram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Signal from Time Domain to Frequency Domain\n",
    "\n",
    "![Time_to_Fre](https://upload.wikimedia.org/wikipedia/commons/6/61/FFT-Time-Frequency-View.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pure_tone(freq, duration=1, sample_rate=44100, volumn=1):\n",
    "    number_samples = duration * sample_rate\n",
    "    samples = np.arange(0, number_samples)\n",
    "    pr = 1 / sample_rate\n",
    "    signal = volumn * np.sin(2 * np.pi * freq * samples * pr)\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 256 # Hz\n",
    "sample_rate = 44100\n",
    "\n",
    "f128 = generate_pure_tone(128, duration=1, sample_rate=sample_rate, volumn=1)\n",
    "f128_offset = generate_pure_tone(128+offset, duration=1, sample_rate=sample_rate, volumn=1)\n",
    "\n",
    "f1024 = generate_pure_tone(1024, duration=1, sample_rate=sample_rate, volumn=0.25)\n",
    "f1024_offset = generate_pure_tone(1024+offset, duration=1, sample_rate=sample_rate, volumn=0.75)\n",
    "\n",
    "# put all signals on top of each other\n",
    "signal_stack = np.array([f128, f128_offset, f1024, f1024_offset])\n",
    "signal_mono = f128 + f128_offset + f1024 + f1024_offset\n",
    "\n",
    "# Put all signals on a sequence\n",
    "signal_sequence = np.concatenate([f128, f128_offset, f1024, f1024_offset])\n",
    "print(signal_sequence.shape)\n",
    "print(signal_stack.shape)\n",
    "print(signal_mono.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(signal: np.ndarray, sample_rate:int, offset:int=0, n_frame:int=None, return_plot:bool=False):\n",
    "    if len(signal.shape) == 1:\n",
    "        signal = np.expand_dims(signal, axis=0)\n",
    "    if n_frame is not None:\n",
    "        signal = signal[:,offset:offset+n_frame]\n",
    "\n",
    "    num_channels, num_frames = signal.shape\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=num_channels, ncols=1)\n",
    "\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        waveshow(signal[c], sr=sample_rate, ax=axes[c])\n",
    "        axes[c].grid(True)\n",
    "        axes[c].label_outer()\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    fig.suptitle(\"waveform\")\n",
    "    if return_plot:\n",
    "        return fig, axes\n",
    "    plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first 1000 frames\n",
    "plot_waveform(signal=signal_stack, sample_rate=sample_rate, offset=0, n_frame=1000)\n",
    "plot_waveform(signal_mono, sample_rate, 0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ipd.Audio(data=signal_mono, rate=sample_rate))\n",
    "display(ipd.Audio(data=signal_stack, rate=sample_rate))\n",
    "display(ipd.Audio(data=signal_sequence, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_magnitude_spectrum(signal: np.ndarray, sample_rate:int, offset:int=0, n_frame:int=None, f_ratio=1):\n",
    "    if len(signal.shape) == 1:\n",
    "        signal = np.expand_dims(signal, axis=0)\n",
    "    if n_frame is not None:\n",
    "        signal = signal[:,offset:offset+n_frame]\n",
    "    \n",
    "    # Apply FFT & Calculate the Magnitude Spectrum\n",
    "    ft = np.fft.fft(signal)\n",
    "    magnitude_spectrum = np.abs(ft)\n",
    "\n",
    "    num_channels, num_frames = signal.shape\n",
    "    figure, axes = plt.subplots(nrows=num_channels, ncols=1)\n",
    "\n",
    "    frequency = np.linspace(0, sample_rate, num_frames)\n",
    "    num_frequency_bins = int(len(frequency)*f_ratio)\n",
    "\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(frequency[:num_frequency_bins], magnitude_spectrum[c,:num_frequency_bins])\n",
    "        axes[c].set_xlabel('Frequency (Hz)')\n",
    "        axes[c].grid(True)\n",
    "        axes[c].label_outer()\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"Magnitude Spectrum\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_magnitude_spectrum(signal_mono, sample_rate, f_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stft_params = {\n",
    "    'n_fft': 1024,\n",
    "    'win_length': None,\n",
    "    'hop_length': 512,\n",
    "    'window': \"hann\",\n",
    "    'center': True,\n",
    "    'dtype': None,\n",
    "    'pad_mode': \"constant\",\n",
    "}\n",
    "\n",
    "S = librosa.feature.melspectrogram(y=signal_sequence, sr=sample_rate, **stft_params, n_mels=108)\n",
    "S_log = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_title('mel spectrogram')\n",
    "im = specshow(S_log, sr=sample_rate, x_axis='time', y_axis='mel', ax=ax)\n",
    "fig.colorbar(im, ax=ax, format=\"%+2.0f dB\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = '../medleydb/medleydb/data/Audio/Phoenix_ScotchMorris/Phoenix_ScotchMorris_MIX.wav'\n",
    "metadata = torchaudio.info(audio_path)\n",
    "song_duration = metadata.num_frames/metadata.sample_rate # in seconds\n",
    "m,s = divmod(song_duration, 60)\n",
    "print(metadata)\n",
    "print(f'Song duration: {song_duration:.2f}s ({m:.0f}m{s:.0f}s)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display Audio\n",
    "ipd.Audio(filename=audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio on memory using Librosa\n",
    "# Note:\n",
    "# - By default, librosa.load() resample audio to 22050Hz by default.\n",
    "# - Output is a mono waveform.\n",
    "librosa_waveform, sr = librosa.load(path=audio_path, sr=metadata.sample_rate)\n",
    "print(librosa_waveform.shape)\n",
    "print(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio on memory using pytorch\n",
    "# Note:\n",
    "# - Unlike librosa, pytorch use the sampling rate stored in the file.\n",
    "# - Include multiple channels\n",
    "# - Output a torch array, which can be converted into numpy.\n",
    "\n",
    "# For Windows user: if torchaudio cannot load `.mp3` format, use librosa instead.\n",
    "pytorch_waveform, sr = torchaudio.load(audio_path)\n",
    "print(pytorch_waveform.shape)\n",
    "print(sr)\n",
    "#convert to mono waveform using mean():\n",
    "waveform_mono = torch.mean(pytorch_waveform, dim=0)\n",
    "print(f'1:1 comparison with Librosa waveform:', 'Matched' if np.all(waveform_mono.numpy() == librosa_waveform) else 'Unmatched')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_waveform(pytorch_waveform.numpy(),sr, 0, 5*sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnitude Spectrum of the whole song\n",
    "plot_magnitude_spectrum(pytorch_waveform.numpy(), sr, f_ratio=1)\n",
    "# Note:\n",
    "# - The Frequency distribution is repeated and mirrored, this is called the redundancies\n",
    "# - To remove the redundancies, simply set `f_ratio=0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we take the specstrum of the whole song, it doesn't give us much info\n",
    "# Introducing Short time fourier transform\n",
    "offset = 0\n",
    "n_frame = 3*sr\n",
    "signal = librosa_waveform[offset:offset+n_frame]\n",
    "\n",
    "spec = librosa.stft(y=signal, **stft_params)\n",
    "magnitude = librosa.amplitude_to_db(np.abs(spec), ref=np.max)\n",
    "print(magnitude.shape)\n",
    "\n",
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_title('Power spectrogram')\n",
    "im = specshow(magnitude, sr=sr, x_axis='time', y_axis='linear', ax=ax)\n",
    "fig.colorbar(im, ax=ax, format=\"%+2.0f dB\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = librosa.feature.melspectrogram(y=signal, sr=sr, **stft_params, n_mels=108)\n",
    "log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "print(mel_spec.shape)\n",
    "print(log_mel_spec.shape)\n",
    "# plot the spectrogram\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.set_title('mel spectrogram')\n",
    "im = specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', ax=ax)\n",
    "fig.colorbar(im, ax=ax, format=\"%+2.0f dB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3071ef67f0d52b8c9e2a13540b1ce413279ebac2bba14c7b121f8f9e6920f86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
