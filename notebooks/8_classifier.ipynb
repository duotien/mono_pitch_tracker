{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "\n",
    "from pitch_tracker.utils.constants import (F_MIN, HOP_LENGTH, N_CLASS, N_FFT,\n",
    "                                           N_MELS, PATCH_SIZE,\n",
    "                                           PATCH_STEP,\n",
    "                                           PATCH_TIME, SAMPLE_RATE,\n",
    "                                           ANALYSIS_FRAME_SIZE, ANALYSIS_FRAME_TIME, WIN_LENGTH)\n",
    "from pitch_tracker.utils.dataset import AudioDataset\n",
    "from pitch_tracker.ml.net import create_conv2d_block, MPT2023\n",
    "from pitch_tracker.ml.train_model import train_model, train, test\n",
    "from pitch_tracker import THESIS_2023_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() \\\n",
    "    else \"mps\" if torch.backends.mps.is_available() \\\n",
    "    else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../pitch_tracker/saved_model/mpt_2023.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union\n",
    "import numpy as np\n",
    "from pitch_tracker.utils.audio import load_audio_mono\n",
    "from pitch_tracker.utils.constants import PRE_MIDI_START\n",
    "from pitch_tracker.utils.dataset import build_pick_features_and_time, extract_melspectrogram_feature\n",
    "\n",
    "class MelodyExtractor():\n",
    "    def __init__(self, model_path=THESIS_2023_MODEL_PATH, device='cpu') -> None:\n",
    "        self.model = MPT2023().to(device)\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            file_path: str = None,\n",
    "            signal: Union[torch.Tensor, np.ndarray] = None,\n",
    "            sample_rate: int = SAMPLE_RATE,\n",
    "            n_fft: int = N_FFT,\n",
    "            n_mels: int = N_MELS*2,\n",
    "            hop_length: int = HOP_LENGTH,\n",
    "            patch_size: int = PATCH_SIZE,\n",
    "            analysis_frame_size: int = ANALYSIS_FRAME_SIZE,\n",
    "            analysis_frame_time: float = ANALYSIS_FRAME_TIME,\n",
    "            fmin:float = F_MIN,\n",
    "            voicing_bias:float = 0.0,\n",
    "    ) -> Any:\n",
    "        \n",
    "        pick_features, pick_times = self.get_pick_features_and_time(\n",
    "            file_path=file_path,\n",
    "            signal=signal,\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            n_mels=n_mels,\n",
    "            hop_length=hop_length,\n",
    "            patch_size=patch_size,\n",
    "            analysis_frame_size=analysis_frame_size,\n",
    "            analysis_frame_time=analysis_frame_time,\n",
    "            fmin=fmin,\n",
    "        )\n",
    "\n",
    "        self.model.eval()\n",
    "        pred = self.model(pick_features)\n",
    "        pred[:,:,0] -= voicing_bias\n",
    "        pitch = pred.argmax(2).flatten()\n",
    "        pitch[pitch>0] += PRE_MIDI_START\n",
    "\n",
    "        pick_times = pick_times.flatten()\n",
    "\n",
    "        return pitch, pick_times\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_pick_features_and_time(\n",
    "            self,\n",
    "            file_path: str = None,\n",
    "            signal: Union[torch.Tensor, np.ndarray] = None,\n",
    "            sample_rate: int = SAMPLE_RATE,\n",
    "            n_fft: int = N_FFT,\n",
    "            n_mels: int = N_MELS*2,\n",
    "            hop_length: int = HOP_LENGTH,\n",
    "            patch_size: int = PATCH_SIZE,\n",
    "            analysis_frame_size: int = ANALYSIS_FRAME_SIZE,\n",
    "            analysis_frame_time: float = ANALYSIS_FRAME_TIME,\n",
    "            fmin:float = F_MIN):\n",
    "        \n",
    "        \n",
    "        if file_path is None and signal is None:\n",
    "            raise Exception('Missing one required parameter `file_path` or `signal`.')\n",
    "        \n",
    "        # ignore `signal` param if file_path is used\n",
    "        if file_path:\n",
    "            signal, _ = load_audio_mono(file_path, sample_rate, keep_channel_dim=False)\n",
    "        \n",
    "        melspectrogram_features = extract_melspectrogram_feature(\n",
    "            y=signal,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            sample_rate=sample_rate,\n",
    "            backend='librosa',\n",
    "            fmin=fmin,\n",
    "        )\n",
    "\n",
    "        pick_features, pick_times = build_pick_features_and_time(\n",
    "            STFT_features=melspectrogram_features.T,\n",
    "            patch_step=patch_size,\n",
    "            patch_size= patch_size,\n",
    "            analysis_frame_size=analysis_frame_size,\n",
    "            analysis_frame_time=analysis_frame_time\n",
    "        )\n",
    "\n",
    "        pick_features = torch.from_numpy(pick_features).type(torch.float32)\n",
    "        pick_times = torch.from_numpy(pick_times).type(torch.float32)\n",
    "\n",
    "        pick_features=pick_features.unsqueeze(1)\n",
    "        return pick_features, pick_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]),\n",
       " tensor([1, 2, 3]),\n",
       " tensor([4]),\n",
       " tensor([5, 6]),\n",
       " tensor([7]),\n",
       " tensor([8, 9]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def consecutive(data:Union[np.ndarray,torch.Tensor], stepsize:int=1, as_indices:bool=False):\n",
    "    if isinstance(data,np.ndarray):\n",
    "        split_indices = np.where(np.diff(data) != stepsize)[0]+1\n",
    "        if as_indices:\n",
    "            data_indices = np.arange(data.shape[0])\n",
    "            return np.split(data_indices, split_indices)\n",
    "        return np.split(data, split_indices)\n",
    "    \n",
    "    if isinstance(data,torch.Tensor):\n",
    "        split_indices = torch.where(torch.diff(data) != stepsize)[0]+1\n",
    "        split_indices = split_indices.tolist()\n",
    "        if as_indices:\n",
    "            data_indices = torch.arange(data.shape[0])\n",
    "            return torch.hsplit(data_indices, split_indices)\n",
    "        return torch.hsplit(data, split_indices)\n",
    "\n",
    "    raise TypeError('`data` must be np.ndarray or torch.Tensor')\n",
    "    \n",
    "\n",
    "def get_consecutive_pred(pitch_pred:torch.Tensor):\n",
    "    \n",
    "    split_indices = torch.where(torch.diff(pitch_pred) != 0)[0]+1\n",
    "    split_indices = split_indices.tolist()\n",
    "    pitch_pred_indices_mask = torch.arange(pitch_pred.shape[0])\n",
    "\n",
    "    sections = torch.hsplit(pitch_pred_indices_mask, split_indices)\n",
    "    sections_pitch_values = pitch_pred[[indices[0] for indices in sections]].tolist()\n",
    "    sections_pitch_values = tuple(sections_pitch_values)\n",
    "\n",
    "    return list(zip(sections, sections_pitch_values))\n",
    "\n",
    "\n",
    "\n",
    "a = torch.Tensor([0, 47, 48, 49, 50, 97, 98, 99])\n",
    "b = torch.Tensor([0, 1, 1, 1, 2, 3, 3, 99, 1,1])\n",
    "consecutive(b, 0, as_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pitch_tracker.utils.midi import build_note_messages\n",
    "from pitch_tracker.utils.midi import convert_to_midi\n",
    "import mido\n",
    "\n",
    "\n",
    "def build_note_sequences(pitch_pred:torch.Tensor, analysis_frame_time:int, analysis_frame_powers=None):\n",
    "    note_sequences = []\n",
    "    pitch_sequences = get_consecutive_pred(pitch_pred)\n",
    "    if analysis_frame_powers is None:\n",
    "        analysis_frame_powers = 50\n",
    "    # filter non-melody sequences\n",
    "    \n",
    "    pitch_sequences = [(sequence, midi_value) for sequence, midi_value in pitch_sequences if midi_value != 0]\n",
    "    for sequence, midi_value in pitch_sequences:\n",
    "        sequence += 1\n",
    "        start_time = (sequence[0] * analysis_frame_time).item()\n",
    "        end_time = (sequence[-1] * analysis_frame_time).item() + analysis_frame_time\n",
    "        note_sequences.append((start_time, end_time, midi_value, analysis_frame_powers))\n",
    "\n",
    "    return torch.Tensor(note_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "melody_extractor = MelodyExtractor(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = [\n",
    "    '../medleydb/medleydb/data/Audio/FamilyBand_Again/FamilyBand_Again_MIX.wav',\n",
    "    '../medleydb/medleydb/data/Audio/AClassicEducation_NightOwl/AClassicEducation_NightOwl_MIX.wav',\n",
    "    '../content/audio/mp3/Take on Me_ORIGINAL.mp3',\n",
    "    '../content/audio/mp3/Let It Happen_Original.mp3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../medleydb/medleydb/data/Audio/FamilyBand_Again/FamilyBand_Again_MIX.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../medleydb/medleydb/data/Audio/AClassicEducation_NightOwl/AClassicEducation_NightOwl_MIX.wav\n",
      "../content/audio/mp3/Take on Me_ORIGINAL.mp3\n",
      "../content/audio/mp3/Let It Happen_Original.mp3\n"
     ]
    }
   ],
   "source": [
    "out_midi_dir = '../content/midi'\n",
    "voicing_bias = 0.2\n",
    "for audio_path in audio_paths:\n",
    "    print(audio_path)\n",
    "    file_name_without_ext = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    out_midi_path = os.path.join(out_midi_dir,file_name_without_ext + '.mid')\n",
    "\n",
    "    pitch, time1d = melody_extractor(audio_path, voicing_bias=0.1)\n",
    "    note_sequences = build_note_sequences(pitch, ANALYSIS_FRAME_TIME)\n",
    "    note_messages = build_note_messages(note_sequences, ticks_per_beat=960)\n",
    "    midi = convert_to_midi(note_messages.numpy(), ticks_per_beat=960)\n",
    "    midi.save(out_midi_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
