{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duotien/mono_pitch_tracker/blob/main/notebooks/mono_pitch_tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rMOhtGFfjOS2"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah605VRzD212",
        "outputId": "9048d825-414c-4426-8c63-fc96d05a42a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Chat-xKbjYey"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone --recursive https://github.com/duotien/mono_pitch_tracker.git\n",
        "!apt -qq install -y sox\n",
        "!pip install pyyaml==5.4.1 sox librosa==0.9.2 torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Get file from google drive...')\n",
        "import shutil\n",
        "file_to_copy_path = '/content/drive/MyDrive/DataBase/MedleyDB/V1_MIX_AUDIO_ONLY.zip'\n",
        "file_to_paste_path = '/content/V1_MIX_AUDIO_ONLY.zip'\n",
        "\n",
        "shutil.copy2(file_to_copy_path, file_to_paste_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuDbVWOJ5eAR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -o V1_MIX_AUDIO_ONLY.zip -d /content/mono_pitch_tracker/medleydb/medleydb/data/Audio/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-4ji2Zj1Frf"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "NOTEBOOK_DIR = os.getcwd()\n",
        "MONO_PITCH_TRACKER_DIR = os.path.join(os.getcwd(), 'mono_pitch_tracker/')\n",
        "MEDLEYDB_DIR = os.path.join(MONO_PITCH_TRACKER_DIR, 'medleydb/')\n",
        "\n",
        "sys.path.append(MONO_PITCH_TRACKER_DIR)\n",
        "sys.path.append(MEDLEYDB_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# change working directory to `mono_pitch_tracker/``\n",
        "%cd $MONO_PITCH_TRACKER_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYsH3fCf2v_l",
        "outputId": "66056720-9746-484a-8a0a-978838aa863a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/mono_pitch_tracker/medleydb/medleydb/__init__.py:69: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  INST_TAXONOMY = yaml.load(fhandle)\n",
            "/content/mono_pitch_tracker/medleydb/medleydb/__init__.py:77: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  MIXING_COEFFICIENTS = yaml.load(fhandle)\n"
          ]
        }
      ],
      "source": [
        "# cut audio into frames\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yaml\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import medleydb\n",
        "from pitch_tracker.utils.constants import (F_MIN, HOP_LENGTH, N_FFT, N_MELS,\n",
        "                                           PATCH_SIZE,\n",
        "                                           PATCH_STEP,\n",
        "                                           PATCH_TIME, SAMPLE_RATE,\n",
        "                                           ANALYSIS_FRAME_SIZE, ANALYSIS_FRAME_TIME, WIN_LENGTH,\n",
        "                                           N_CLASS, RANDOM_STATE)\n",
        "from pitch_tracker.utils.audio import load_audio_mono\n",
        "from pitch_tracker.utils.files import get_file_name, list_file_paths_in_dir, list_folder_paths_in_dir\n",
        "\n",
        "# from pitch_tracker.utils import dataset, files\n",
        "from pitch_tracker.utils.medleydb_melody import gen_label\n",
        "from pitch_tracker.utils import dataset\n",
        "from pitch_tracker.utils.dataset import AudioDataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWO4w3XhKl_a"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stft_hop_size = 512\n",
        "analysis_frame_size = 5\n",
        "n_mels = 88\n",
        "onset_frame_time = stft_hop_size*analysis_frame_size/SAMPLE_RATE\n",
        "pick_frame_time = PATCH_SIZE * onset_frame_time\n",
        "\n",
        "DATASET_DIR = os.path.join(MONO_PITCH_TRACKER_DIR,f'content/pickled_database/{stft_hop_size}_{analysis_frame_size}_{n_mels}/')\n",
        "DATA_SPLIT_PATH = dataset.DATA_SPLIT_PATH\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare datset\n",
        "!python scripts/prepare_dataset.py --stft_hop_size {stft_hop_size} --analysis_frame_size {analysis_frame_size} --n_mels {n_mels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function takes in several arguments to control how the dataset is split. The by argument specifies the method used to split the data:\n",
        "# If by is set to 'song_name': the data is randomized and split so that some samples from one song can be present in all three sets.\n",
        "# If by is set to 'basaran2018CRNN': the data is split by artist according to a pre-defined split.\n",
        "# If by is set to 'thesis': the data is split by genre with a test set identical to that of 'basaran2018CRNN' for comparison.\n",
        "train_df, validation_df, test_df = dataset.split_dataset_df('thesis', pickled_data_dir=DATASET_DIR)\n",
        "\n",
        "train_set, validation_set, test_set = train_df['pickled_path'], validation_df['pickled_path'], test_df['pickled_path']\n",
        "\n",
        "train_dataset, validation_dataset, test_dataset = AudioDataset(train_set), AudioDataset(validation_set), AudioDataset(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3ud7hLbWe6x"
      },
      "source": [
        "# Modeling & training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pitch_tracker.ml.net import Audio_CNN, Audio_CRNN, create_conv2d_block, conv2d_output_shape\n",
        "from pitch_tracker.ml.train_model import train_model\n",
        "from pitch_tracker.ml.earlystopping import EarlyStopping\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model_1_512_5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_1_512_5, self).__init__()\n",
        "        self.conv2d_block1 = create_conv2d_block(\n",
        "            conv2d_input=(1,128,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=None,\n",
        "        )\n",
        "        \n",
        "        self.conv2d_block2 = create_conv2d_block(\n",
        "            conv2d_input=(128,128,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=(1,5),\n",
        "        )\n",
        "\n",
        "        self.conv2d_block3 = create_conv2d_block(\n",
        "            conv2d_input=(128,64,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=(1,2),\n",
        "        )\n",
        "\n",
        "        self.flatten_layer = nn.Flatten(start_dim=2)\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=512,\n",
        "            hidden_size=128,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "\n",
        "        self.gru_bidirectional = nn.GRU(\n",
        "            input_size=128,\n",
        "            hidden_size=64,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.maxpool1d = nn.MaxPool1d(\n",
        "            kernel_size=5,\n",
        "        )\n",
        "        self.dense_layer = nn.LazyLinear(128)\n",
        "        self.output_layer = nn.LazyLinear(88)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv2d_block1(x)\n",
        "        x = self.conv2d_block2(x)\n",
        "        x = self.conv2d_block3(x)\n",
        "        x = x.permute((0,2,3,1)) # [batch, channel, n_frames, n_mel] -> [batch, n_frames, n_mel * channel]\n",
        "        x = self.flatten_layer(x)\n",
        "        x, h_n = self.gru(x)\n",
        "        x, h_n = self.gru_bidirectional(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.maxpool1d(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.dense_layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model_2_512_5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_2_512_5, self).__init__()\n",
        "        self.conv2d_block1 = create_conv2d_block(\n",
        "            conv2d_input=(1,256,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=None,\n",
        "        )\n",
        "        \n",
        "        self.conv2d_block2 = create_conv2d_block(\n",
        "            conv2d_input=(256,128,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=(1,5),\n",
        "        )\n",
        "\n",
        "        self.conv2d_block3 = create_conv2d_block(\n",
        "            conv2d_input=(128,64,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=(1,5),\n",
        "        )\n",
        "        \n",
        "        self.flatten_layer = nn.Flatten(start_dim=2)\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=192,\n",
        "            hidden_size=128,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "\n",
        "        self.gru_bidirectional = nn.GRU(\n",
        "            input_size=128,\n",
        "            hidden_size=128,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.maxpool1d = nn.MaxPool1d(\n",
        "            kernel_size=5,\n",
        "        )\n",
        "        self.dense_layer = nn.LazyLinear(128)\n",
        "        self.output_layer = nn.LazyLinear(88)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv2d_block1(x)\n",
        "        x = self.conv2d_block2(x)\n",
        "        x = self.conv2d_block3(x)\n",
        "        # x = self.conv2d_block4(x)\n",
        "        x = x.permute((0,2,3,1)) # [batch, channel, n_frames, n_mel] -> [batch, n_frames, n_mel * channel]\n",
        "        # print(x.is_contiguous())\n",
        "        x = self.flatten_layer(x)\n",
        "        x, h_n = self.gru(x)\n",
        "        x, h_n = self.gru_bidirectional(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.maxpool1d(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.dense_layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Model_1_512_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model_1_512_2, self).__init__()\n",
        "        self.conv2d_block1 = create_conv2d_block(\n",
        "            conv2d_input=(1,128,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=None,\n",
        "        )\n",
        "        \n",
        "        self.conv2d_block2 = create_conv2d_block(\n",
        "            conv2d_input=(128,128,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=(1,5),\n",
        "        )\n",
        "\n",
        "        self.conv2d_block3 = create_conv2d_block(\n",
        "            conv2d_input=(128,64,3),\n",
        "            padding='same',\n",
        "            maxpool_kernel_size=(1,2),\n",
        "        )\n",
        "\n",
        "        self.flatten_layer = nn.Flatten(start_dim=2)\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=512,\n",
        "            hidden_size=128,\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "\n",
        "        self.gru_bidirectional = nn.GRU(\n",
        "            input_size=128,\n",
        "            hidden_size=64,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.maxpool1d = nn.MaxPool1d(\n",
        "            kernel_size=5,\n",
        "        )\n",
        "        self.dense_layer = nn.LazyLinear(128)\n",
        "        self.output_layer = nn.LazyLinear(88)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv2d_block1(x)\n",
        "        x = self.conv2d_block2(x)\n",
        "        x = self.conv2d_block3(x)\n",
        "        x = x.permute((0,2,3,1)) # [batch, channel, n_frames, n_mel] -> [batch, n_frames, n_mel * channel]\n",
        "        x = self.flatten_layer(x)\n",
        "        x, h_n = self.gru(x)\n",
        "        x, h_n = self.gru_bidirectional(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.maxpool1d(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.dense_layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model_1_512_2()\n",
        "dummy_in_shape = [1] + list(train_dataset.__getitem__(0)[0].shape)\n",
        "dummy_in = torch.randn(dummy_in_shape)\n",
        "print(f'Input size: {tuple(dummy_in.shape)}')\n",
        "print(f'Output size: {tuple(model(dummy_in).shape)}')\n",
        "del dummy_in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(model, dummy_in_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() \\\n",
        "    else \"mps\" if torch.backends.mps.is_available() \\\n",
        "    else \"cpu\"\n",
        "print(f\"Using {DEVICE} device\")\n",
        "\n",
        "# create config file\n",
        "p = {\n",
        "    # dataset\n",
        "    'batch_size': 8,\n",
        "    # fit\n",
        "    'n_epochs': 100,\n",
        "    'learning_rate': 1e-3,\n",
        "    # early stopping\n",
        "    'es_patience': 10,\n",
        "    'es_verbose': True,\n",
        "    'es_dir_path': './checkpoints',\n",
        "    # lr scheduler\n",
        "    'ls_patience': 4,\n",
        "    'ls_factor': 0.2,\n",
        "    # misc\n",
        "    'device': DEVICE,\n",
        "}\n",
        "\n",
        "with open('./scripts/config/model_config.yml', 'w') as f:\n",
        "    yaml.dump(p,f,sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create config file\n",
        "p = {\n",
        "    # dataset\n",
        "    'batch_size': 16,\n",
        "    # fit\n",
        "    'n_epochs': 100,\n",
        "    'learning_rate': 1e-3,\n",
        "    # early stopping\n",
        "    'es_patience': 10,\n",
        "    'es_verbose': True,\n",
        "    'es_dir_path': './checkpoints',\n",
        "    # lr scheduler\n",
        "    'ls_patience': 6,\n",
        "    'ls_factor': 0.2,\n",
        "    # misc\n",
        "    'device': 'DEVICE',\n",
        "}\n",
        "\n",
        "with open('./scripts/config/model_config.yml', 'w') as f:\n",
        "    yaml.dump(p,f,sort_keys=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=p['batch_size'],\n",
        "    shuffle=True\n",
        ")\n",
        "    \n",
        "validation_dataloader = DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=p['batch_size'],\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BCE loss doesn't work well.\n",
        "loss_fn = nn.CrossEntropyLoss().to(p['device'])\n",
        "model = model.to(p['device'])\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=p['learning_rate']\n",
        ")\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=p['es_patience'],\n",
        "    verbose=p['es_verbose'],\n",
        "    dir_path=p['es_dir_path']\n",
        ")\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    optimizer=optimizer,\n",
        "    patience=p['ls_patience'],\n",
        "    factor=p['ls_factor'],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, avg_train_losses, avg_validation_losses = train_model(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    validation_dataloader=validation_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer=optimizer,\n",
        "    n_epochs=p['n_epochs'],\n",
        "    early_stopping=early_stopping,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    device=p['device'],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train model\n",
        "# ! python scripts/model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhM3QWfKChSJ"
      },
      "outputs": [],
      "source": [
        "model = torch.load('/content/drive/MyDrive/School work/KLTN/2022-2023/models/mpt_v01_20231028-2010.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daT0fYRvKetg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "dummy_in = torch.randn(8, 1, 1050, 88)\n",
        "in_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
        "out_names = [ \"output1\" ]\n",
        "\n",
        "torch.onnx.export(model, dummy_in, f=\"model.onnx\", input_names=in_names, output_names=out_names, opset_version=7, verbose=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNzm/ZmdkfbNA1ZwHXLOXBi",
      "include_colab_link": true,
      "mount_file_id": "1xxtvjbt8f5apGe2geu0_Nt7ldY4N7yAo",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
