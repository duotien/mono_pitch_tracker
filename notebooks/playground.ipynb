{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tien.d/workspace/GITHUB/mono_pitch_tracker/medleydb/medleydb/__init__.py:69: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  INST_TAXONOMY = yaml.load(fhandle)\n",
      "/Users/tien.d/workspace/GITHUB/mono_pitch_tracker/medleydb/medleydb/__init__.py:77: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  MIXING_COEFFICIENTS = yaml.load(fhandle)\n"
     ]
    }
   ],
   "source": [
    "from pitch_tracker.ml.net import Audio_CNN, Audio_CRNN, create_conv2d_block, conv2d_output_shape\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data.json', 'r') as file:\n",
    "    # Load the JSON data from the file\n",
    "    data = json.load(file)\n",
    "\n",
    "# Now you can access the data as a normal Python dictionary\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Test_Model, self).__init__()\n",
    "        self.conv2d_block1 = create_conv2d_block(\n",
    "            conv2d_input=(1,64,3),\n",
    "            padding='same',\n",
    "            maxpool_kernel_size=None,\n",
    "        )\n",
    "        \n",
    "        self.conv2d_block2 = create_conv2d_block(\n",
    "            conv2d_input=(64,64,3),\n",
    "            padding='same',\n",
    "            maxpool_kernel_size=(1,5),\n",
    "        )\n",
    "\n",
    "        self.conv2d_block3 = create_conv2d_block(\n",
    "            conv2d_input=(64,64,3),\n",
    "            padding='same',\n",
    "            maxpool_kernel_size=(1,5),\n",
    "        )\n",
    "        \n",
    "        self.conv2d_block4 = create_conv2d_block(\n",
    "            conv2d_input=(64,210,3),\n",
    "            padding='same',\n",
    "            # maxpool_kernel_size=(1,5),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Flatten(start_dim=2)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=3150,\n",
    "            hidden_size=88,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.dense_layer = nn.Linear(88,88)\n",
    "        # self.output_layer = nn.Linear(128, 88)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv2d_block1(x)\n",
    "        x = self.conv2d_block2(x)\n",
    "        x = self.conv2d_block3(x)\n",
    "        x = self.conv2d_block4(x)\n",
    "        flat = self.flatten_layer(x)\n",
    "        sequence, h_n = self.gru(flat)\n",
    "        out = self.dense_layer(sequence)\n",
    "        # x = self.output_layer(x)\n",
    "        return out\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_Model(\n",
      "  (conv2d_block1): Sequential(\n",
      "    (conv2d): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu): ReLU()\n",
      "    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2d_block2): Sequential(\n",
      "    (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu): ReLU()\n",
      "    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (maxpool2d): MaxPool2d(kernel_size=(1, 5), stride=(1, 5), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2d_block3): Sequential(\n",
      "    (conv2d): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu): ReLU()\n",
      "    (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (maxpool2d): MaxPool2d(kernel_size=(1, 5), stride=(1, 5), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2d_block4): Sequential(\n",
      "    (conv2d): Conv2d(64, 210, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu): ReLU()\n",
      "    (batch_norm): BatchNorm2d(210, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (flatten_layer): Flatten(start_dim=2, end_dim=-1)\n",
      "  (gru): GRU(3150, 88, batch_first=True, dropout=0.2)\n",
      "  (dense_layer): Linear(in_features=88, out_features=88, bias=True)\n",
      ")\n",
      "torch.Size([4, 210, 88])\n"
     ]
    }
   ],
   "source": [
    "model = Test_Model()\n",
    "print(model)\n",
    "dummy_input = torch.randn((4,1,1050,88))\n",
    "out = model(dummy_input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Test_Model                               [4, 210, 88]              --\n",
       "├─Sequential: 1-1                        [4, 64, 1050, 88]         --\n",
       "│    └─Conv2d: 2-1                       [4, 64, 1050, 88]         640\n",
       "│    └─ReLU: 2-2                         [4, 64, 1050, 88]         --\n",
       "│    └─BatchNorm2d: 2-3                  [4, 64, 1050, 88]         128\n",
       "├─Sequential: 1-2                        [4, 64, 1050, 17]         --\n",
       "│    └─Conv2d: 2-4                       [4, 64, 1050, 88]         36,928\n",
       "│    └─ReLU: 2-5                         [4, 64, 1050, 88]         --\n",
       "│    └─BatchNorm2d: 2-6                  [4, 64, 1050, 88]         128\n",
       "│    └─MaxPool2d: 2-7                    [4, 64, 1050, 17]         --\n",
       "├─Sequential: 1-3                        [4, 64, 1050, 3]          --\n",
       "│    └─Conv2d: 2-8                       [4, 64, 1050, 17]         36,928\n",
       "│    └─ReLU: 2-9                         [4, 64, 1050, 17]         --\n",
       "│    └─BatchNorm2d: 2-10                 [4, 64, 1050, 17]         128\n",
       "│    └─MaxPool2d: 2-11                   [4, 64, 1050, 3]          --\n",
       "├─Sequential: 1-4                        [4, 210, 1050, 3]         --\n",
       "│    └─Conv2d: 2-12                      [4, 210, 1050, 3]         121,170\n",
       "│    └─ReLU: 2-13                        [4, 210, 1050, 3]         --\n",
       "│    └─BatchNorm2d: 2-14                 [4, 210, 1050, 3]         420\n",
       "├─Flatten: 1-5                           [4, 210, 3150]            --\n",
       "├─GRU: 1-6                               [4, 210, 88]              855,360\n",
       "├─Linear: 1-7                            [4, 210, 88]              7,832\n",
       "==========================================================================================\n",
       "Total params: 1,059,662\n",
       "Trainable params: 1,059,662\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 18.77\n",
       "==========================================================================================\n",
       "Input size (MB): 1.48\n",
       "Forward/backward pass size (MB): 873.57\n",
       "Params size (MB): 4.24\n",
       "Estimated Total Size (MB): 879.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(4, 1, 1050, 88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {\n",
    "    # dataset\n",
    "    'batch_size': 4,\n",
    "    # fit\n",
    "    'n_epochs': 5,\n",
    "    'learning_rate': 1e-3,\n",
    "    # early stopping\n",
    "    'es_patience': 10,\n",
    "    'es_verbose': True,\n",
    "    'es_dir_path': './checkpoints',\n",
    "    # lr scheduler\n",
    "    'ls_patience': 8,\n",
    "    'ls_factor': 0.2,\n",
    "    # misc\n",
    "    'device': 'DEVICE',\n",
    "}\n",
    "\n",
    "with open('config.yaml', 'w') as f:\n",
    "    yaml.dump(p,f,sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch IR graph at exception: graph(%input.1 : Float(4, 1, 1050, 88, strides=[92400, 92400, 88, 1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block1.conv2d.weight : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block1.conv2d.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block1.batch_norm.weight : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block1.batch_norm.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block1.batch_norm.running_mean : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block1.batch_norm.running_var : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block1.batch_norm.num_batches_tracked : Long(requires_grad=0, device=cpu),\n",
      "      %conv2d_block2.conv2d.weight : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block2.conv2d.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block2.batch_norm.weight : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block2.batch_norm.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block2.batch_norm.running_mean : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block2.batch_norm.running_var : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block2.batch_norm.num_batches_tracked : Long(requires_grad=0, device=cpu),\n",
      "      %conv2d_block3.conv2d.weight : Float(64, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block3.conv2d.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block3.batch_norm.weight : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block3.batch_norm.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block3.batch_norm.running_mean : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block3.batch_norm.running_var : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block3.batch_norm.num_batches_tracked : Long(requires_grad=0, device=cpu),\n",
      "      %conv2d_block4.conv2d.weight : Float(210, 64, 3, 3, strides=[576, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block4.conv2d.bias : Float(210, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block4.batch_norm.weight : Float(210, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block4.batch_norm.bias : Float(210, strides=[1], requires_grad=1, device=cpu),\n",
      "      %conv2d_block4.batch_norm.running_mean : Float(210, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block4.batch_norm.running_var : Float(210, strides=[1], requires_grad=0, device=cpu),\n",
      "      %conv2d_block4.batch_norm.num_batches_tracked : Long(requires_grad=0, device=cpu),\n",
      "      %gru.weight_ih_l0 : Float(264, 3150, strides=[3150, 1], requires_grad=1, device=cpu),\n",
      "      %gru.weight_hh_l0 : Float(264, 88, strides=[88, 1], requires_grad=1, device=cpu),\n",
      "      %gru.bias_ih_l0 : Float(264, strides=[1], requires_grad=1, device=cpu),\n",
      "      %gru.bias_hh_l0 : Float(264, strides=[1], requires_grad=1, device=cpu),\n",
      "      %dense_layer.weight : Float(88, 88, strides=[88, 1], requires_grad=1, device=cpu),\n",
      "      %dense_layer.bias : Float(88, strides=[1], requires_grad=1, device=cpu)):\n",
      "  %620 : int[] = prim::Constant[value=[1, 1]]()\n",
      "  %488 : str = prim::Constant[value=\"same\"](), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.conv.Conv2d::conv2d # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %636 : Long(device=cpu) = prim::Constant[value={1}](), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.conv.Conv2d::conv2d\n",
      "  %input.3 : Float(4, 64, 1050, 88, strides=[5913600, 92400, 88, 1], requires_grad=1, device=cpu) = aten::_convolution_mode(%input.1, %conv2d_block1.conv2d.weight, %conv2d_block1.conv2d.bias, %620, %488, %620, %636), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.conv.Conv2d::conv2d # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %input.5 : Float(4, 64, 1050, 88, strides=[5913600, 92400, 88, 1], requires_grad=1, device=cpu) = aten::relu(%input.3), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.activation.ReLU::relu # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %637 : Bool(device=cpu) = prim::Constant[value={0}](), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm\n",
      "  %638 : Double(device=cpu) = prim::Constant[value={0.1}](), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm\n",
      "  %639 : Double(device=cpu) = prim::Constant[value={1e-05}](), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm\n",
      "  %640 : Bool(device=cpu) = prim::Constant[value={1}](), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm\n",
      "  %input.7 : Float(4, 64, 1050, 88, strides=[5913600, 92400, 88, 1], requires_grad=1, device=cpu) = aten::batch_norm(%input.5, %conv2d_block1.batch_norm.weight, %conv2d_block1.batch_norm.bias, %conv2d_block1.batch_norm.running_mean, %conv2d_block1.batch_norm.running_var, %637, %638, %639, %640), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block1/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:2450:0\n",
      "  %input.9 : Float(4, 64, 1050, 88, strides=[5913600, 92400, 88, 1], requires_grad=1, device=cpu) = aten::_convolution_mode(%input.7, %conv2d_block2.conv2d.weight, %conv2d_block2.conv2d.bias, %620, %488, %620, %636), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block2/torch.nn.modules.conv.Conv2d::conv2d # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %input.11 : Float(4, 64, 1050, 88, strides=[5913600, 92400, 88, 1], requires_grad=1, device=cpu) = aten::relu(%input.9), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block2/torch.nn.modules.activation.ReLU::relu # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %514 : Float(4, 64, 1050, 88, strides=[5913600, 92400, 88, 1], requires_grad=1, device=cpu) = aten::batch_norm(%input.11, %conv2d_block2.batch_norm.weight, %conv2d_block2.batch_norm.bias, %conv2d_block2.batch_norm.running_mean, %conv2d_block2.batch_norm.running_var, %637, %638, %639, %640), scope: __main__.Test_Model::/torch.nn.modules.containe"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::_convolution_mode' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m in_names \u001b[39m=\u001b[39m [ \u001b[39m\"\u001b[39m\u001b[39mactual_input_1\u001b[39m\u001b[39m\"\u001b[39m ] \u001b[39m+\u001b[39m [ \u001b[39m\"\u001b[39m\u001b[39mlearned_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m16\u001b[39m) ]\n\u001b[1;32m      4\u001b[0m out_names \u001b[39m=\u001b[39m [ \u001b[39m\"\u001b[39m\u001b[39moutput1\u001b[39m\u001b[39m\"\u001b[39m ]\n\u001b[0;32m----> 6\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m      7\u001b[0m     model,\n\u001b[1;32m      8\u001b[0m     dummy_input,\n\u001b[1;32m      9\u001b[0m     f\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdummy_model.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     \u001b[39m# input_names=in_names,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m     output_names\u001b[39m=\u001b[39;49mout_names,\n\u001b[1;32m     12\u001b[0m     opset_version\u001b[39m=\u001b[39;49m\u001b[39m14\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     do_constant_folding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/onnx/utils.py:504\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    188\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    205\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \n\u001b[1;32m    208\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m     _export(\n\u001b[1;32m    505\u001b[0m         model,\n\u001b[1;32m    506\u001b[0m         args,\n\u001b[1;32m    507\u001b[0m         f,\n\u001b[1;32m    508\u001b[0m         export_params,\n\u001b[1;32m    509\u001b[0m         verbose,\n\u001b[1;32m    510\u001b[0m         training,\n\u001b[1;32m    511\u001b[0m         input_names,\n\u001b[1;32m    512\u001b[0m         output_names,\n\u001b[1;32m    513\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    514\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    515\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    516\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    517\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    518\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    519\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    520\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/onnx/utils.py:1529\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1527\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1529\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1530\u001b[0m     model,\n\u001b[1;32m   1531\u001b[0m     args,\n\u001b[1;32m   1532\u001b[0m     verbose,\n\u001b[1;32m   1533\u001b[0m     input_names,\n\u001b[1;32m   1534\u001b[0m     output_names,\n\u001b[1;32m   1535\u001b[0m     operator_export_type,\n\u001b[1;32m   1536\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1537\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1538\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1539\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1542\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1544\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1545\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/onnx/utils.py:1115\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1112\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1114\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     graph \u001b[39m=\u001b[39m _optimize_graph(\n\u001b[1;32m   1116\u001b[0m         graph,\n\u001b[1;32m   1117\u001b[0m         operator_export_type,\n\u001b[1;32m   1118\u001b[0m         _disable_torch_constant_prop\u001b[39m=\u001b[39;49m_disable_torch_constant_prop,\n\u001b[1;32m   1119\u001b[0m         fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1120\u001b[0m         params_dict\u001b[39m=\u001b[39;49mparams_dict,\n\u001b[1;32m   1121\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1122\u001b[0m         input_names\u001b[39m=\u001b[39;49minput_names,\n\u001b[1;32m   1123\u001b[0m         module\u001b[39m=\u001b[39;49mmodule,\n\u001b[1;32m   1124\u001b[0m     )\n\u001b[1;32m   1125\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1126\u001b[0m     torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mTorch IR graph at exception: \u001b[39m\u001b[39m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/onnx/utils.py:663\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    660\u001b[0m     _C\u001b[39m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    661\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 663\u001b[0m graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39;49m_jit_pass_onnx(graph, operator_export_type)\n\u001b[1;32m    664\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    665\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/onnx/utils.py:1909\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1905\u001b[0m     \u001b[39mif\u001b[39;00m namespace \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monnx\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1906\u001b[0m         \u001b[39m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[1;32m   1907\u001b[0m         \u001b[39mreturn\u001b[39;00m graph_context\u001b[39m.\u001b[39mop(op_name, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattrs, outputs\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39moutputsSize())  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1909\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1910\u001b[0m         domain,\n\u001b[1;32m   1911\u001b[0m         op_name,\n\u001b[1;32m   1912\u001b[0m         opset_version,\n\u001b[1;32m   1913\u001b[0m         symbolic_function_group\u001b[39m.\u001b[39mget_min_supported()\n\u001b[1;32m   1914\u001b[0m         \u001b[39mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1915\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1916\u001b[0m     )\n\u001b[1;32m   1918\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[39mif\u001b[39;00m operator_export_type \u001b[39m==\u001b[39m _C_onnx\u001b[39m.\u001b[39mOperatorExportTypes\u001b[39m.\u001b[39mONNX_FALLTHROUGH:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: Exporting the operator 'aten::_convolution_mode' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r.Sequential::conv2d_block2/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:2450:0\n",
      "  %624 : int[] = prim::Constant[value=[1, 5]]()\n",
      "  %626 : int[] = prim::Constant[value=[0, 0]]()\n",
      "  %input.13 : Float(4, 64, 1050, 17, strides=[1142400, 17850, 17, 1], requires_grad=1, device=cpu) = aten::max_pool2d(%514, %624, %624, %626, %620, %637), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block2/torch.nn.modules.pooling.MaxPool2d::maxpool2d # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n",
      "  %input.15 : Float(4, 64, 1050, 17, strides=[1142400, 17850, 17, 1], requires_grad=1, device=cpu) = aten::_convolution_mode(%input.13, %conv2d_block3.conv2d.weight, %conv2d_block3.conv2d.bias, %620, %488, %620, %636), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block3/torch.nn.modules.conv.Conv2d::conv2d # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %input.17 : Float(4, 64, 1050, 17, strides=[1142400, 17850, 17, 1], requires_grad=1, device=cpu) = aten::relu(%input.15), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block3/torch.nn.modules.activation.ReLU::relu # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %543 : Float(4, 64, 1050, 17, strides=[1142400, 17850, 17, 1], requires_grad=1, device=cpu) = aten::batch_norm(%input.17, %conv2d_block3.batch_norm.weight, %conv2d_block3.batch_norm.bias, %conv2d_block3.batch_norm.running_mean, %conv2d_block3.batch_norm.running_var, %637, %638, %639, %640), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block3/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:2450:0\n",
      "  %input.19 : Float(4, 64, 1050, 3, strides=[201600, 3150, 3, 1], requires_grad=1, device=cpu) = aten::max_pool2d(%543, %624, %624, %626, %620, %637), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block3/torch.nn.modules.pooling.MaxPool2d::maxpool2d # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:782:0\n",
      "  %input.21 : Float(4, 210, 1050, 3, strides=[661500, 3150, 3, 1], requires_grad=1, device=cpu) = aten::_convolution_mode(%input.19, %conv2d_block4.conv2d.weight, %conv2d_block4.conv2d.bias, %620, %488, %620, %636), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block4/torch.nn.modules.conv.Conv2d::conv2d # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %input.23 : Float(4, 210, 1050, 3, strides=[661500, 3150, 3, 1], requires_grad=1, device=cpu) = aten::relu(%input.21), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block4/torch.nn.modules.activation.ReLU::relu # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:1457:0\n",
      "  %572 : Float(4, 210, 1050, 3, strides=[661500, 3150, 3, 1], requires_grad=1, device=cpu) = aten::batch_norm(%input.23, %conv2d_block4.batch_norm.weight, %conv2d_block4.batch_norm.bias, %conv2d_block4.batch_norm.running_mean, %conv2d_block4.batch_norm.running_var, %637, %638, %639, %640), scope: __main__.Test_Model::/torch.nn.modules.container.Sequential::conv2d_block4/torch.nn.modules.batchnorm.BatchNorm2d::batch_norm # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/functional.py:2450:0\n",
      "  %641 : Long(device=cpu) = prim::Constant[value={2}](), scope: __main__.Test_Model::/torch.nn.modules.flatten.Flatten::flatten_layer\n",
      "  %642 : Long(device=cpu) = prim::Constant[value={-1}](), scope: __main__.Test_Model::/torch.nn.modules.flatten.Flatten::flatten_layer\n",
      "  %input : Float(4, 210, 3150, strides=[661500, 3150, 1], requires_grad=1, device=cpu) = aten::flatten(%572, %641, %642), scope: __main__.Test_Model::/torch.nn.modules.flatten.Flatten::flatten_layer # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/flatten.py:46:0\n",
      "  %643 : Long(device=cpu) = prim::Constant[value={0}](), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru\n",
      "  %577 : Long(device=cpu) = aten::size(%input, %643), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/rnn.py:939:0\n",
      "  %644 : Long(device=cpu) = prim::Constant[value={88}](), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru\n",
      "  %582 : int[] = prim::ListConstruct(%636, %577, %644), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru\n",
      "  %645 : Long(device=cpu) = prim::Constant[value={6}](), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru\n",
      "  %584 : NoneType = prim::Constant(), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru\n",
      "  %585 : Device = prim::Constant[value=\"cpu\"](), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/rnn.py:945:0\n",
      "  %hidden : Float(1, 4, 88, strides=[352, 88, 1], requires_grad=0, device=cpu) = aten::zeros(%582, %645, %584, %585, %637), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/rnn.py:945:0\n",
      "  %610 : Tensor[] = prim::ListConstruct(%gru.weight_ih_l0, %gru.weight_hh_l0, %gru.bias_ih_l0, %gru.bias_hh_l0), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru\n",
      "  %646 : Double(device=cpu) = prim::Constant[value={0.2}](), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru\n",
      "  %617 : Float(4, 210, 88, strides=[88, 352, 1], requires_grad=1, device=cpu), %618 : Float(1, 4, 88, strides=[352, 88, 1], requires_grad=1, device=cpu) = aten::gru(%input, %hidden, %610, %640, %636, %646, %637, %637, %640), scope: __main__.Test_Model::/torch.nn.modules.rnn.GRU::gru # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/rnn.py:955:0\n",
      "  %619 : Float(4, 210, 88, strides=[18480, 88, 1], requires_grad=1, device=cpu) = aten::linear(%617, %dense_layer.weight, %dense_layer.bias), scope: __main__.Test_Model::/torch.nn.modules.linear.Linear::dense_layer # /Users/tien.d/opt/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%619)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "in_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\n",
    "out_names = [ \"output1\" ]\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    f=\"dummy_model.onnx\",\n",
    "    # input_names=in_names,\n",
    "    output_names=out_names,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3071ef67f0d52b8c9e2a13540b1ce413279ebac2bba14c7b121f8f9e6920f86"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
